---
title: "Statistical Analysis with dsVertClient"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Analysis with dsVertClient}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  eval = FALSE,
  purl = FALSE
)
```

This vignette demonstrates the full statistical analysis workflow in
dsVertClient: computing correlation matrices, performing principal component
analysis (PCA), and fitting generalized linear models (GLMs) across vertically
partitioned data. All of these methods operate on aligned data and preserve
privacy so that raw individual-level data never leaves any server.

## Prerequisites

Before running any statistical analysis, records must be aligned across servers
so that the same row position corresponds to the same patient everywhere.
The [Getting Started](a-getting-started.html) vignette covers connection setup,
identifier validation, and record alignment in detail.

We assume the following environment is already in place:

```{r prerequisites, purl = FALSE}
library(dsVertClient)
library(DSI)

# Three institutions, already connected and with data assigned to "D"
# inst_A holds: age, weight         (plus outcome variables)
# inst_B holds: height, bmi         (plus outcome variables)
# inst_C holds: glucose, cholesterol (plus outcome variables)

# Records have been aligned into "D_aligned" using ds.hashId + ds.alignRecords
# (see the Getting Started vignette)
```

---

## Step 1: Correlation Analysis

### The Challenge

Computing correlations across vertically partitioned data is non-trivial.
Consider computing the correlation between `age` (at Institution A) and
`glucose` (at Institution C):

- Institution A knows `age` values but not `glucose`
- Institution C knows `glucose` values but not `age`
- Neither can compute the correlation alone
- We cannot send raw values between institutions

### The Solution: Multiparty Homomorphic Encryption (MHE)

`ds.vertCor` uses **Multiparty Homomorphic Encryption (MHE)** with threshold
decryption to compute cross-server correlations without exposing raw data. The
protocol proceeds in six phases:

1. **Key Generation** -- Each server generates its own secret key share and a
   public key share. One server also generates a Common Reference Polynomial
   (CRP) shared by all parties.
2. **Key Combination** -- Public key shares are combined into a Collective
   Public Key (CPK). Data encrypted under the CPK can only be decrypted when
   ALL servers cooperate.
3. **Encryption** -- Each server standardizes its columns (Z-scores) and
   encrypts them column-by-column under the CPK using the CKKS approximate
   homomorphic encryption scheme.
4. **Local Correlation** -- Within-server correlations are computed in plaintext
   (no encryption needed for data that stays on one server).
5. **Cross-Server Correlation** -- For each pair of servers (A, B), server A
   receives the encrypted columns from B and computes the element-wise product
   homomorphically. The encrypted result is then partially decrypted by each
   server in turn. The client fuses all partial decryption shares to recover
   the inner product (and thus the correlation coefficient).
6. **Assembly** -- The client assembles the full p x p correlation matrix from
   local correlations (diagonal blocks) and cross-server correlations
   (off-diagonal blocks).

The key security property is that **no single server can decrypt the
cross-server results**. Full decryption requires cooperation from every server.

### Computing the Correlation Matrix

Define which variables reside on which server, then call `ds.vertCor`:

```{r define-variables, purl = FALSE}
variables <- list(
  inst_A = c("age", "weight"),
  inst_B = c("height", "bmi"),
  inst_C = c("glucose", "cholesterol")
)
```

```{r compute-correlation, purl = FALSE}
cor_result <- ds.vertCor(
  data_name = "D_aligned",
  variables = variables,
  log_n = 12,        # CKKS ring dimension (12 = 2048 slots, good for n <= 2048)
  log_scale = 40,    # CKKS precision (~12 decimal digits)
  datasources = connections
)
```

The `log_n` parameter controls the CKKS ring dimension and determines the
maximum number of observations that can be processed:

| `log_n` | Max observations | Speed    |
|---------|-----------------|----------|
| 12      | 2,048           | Fastest  |
| 13      | 4,096           | Moderate |
| 14      | 8,192           | Slowest  |

For most datasets with a few thousand patients, the default `log_n = 12` is
appropriate.

### Viewing the Result

The `print` method provides a formatted summary:

```{r print-correlation, purl = FALSE}
print(cor_result)
#> Correlation Matrix (MHE-CKKS-Threshold)
#> =====================================
#>
#> Observations: 200
#> Variables: 6
#> Servers: inst_A, inst_B, inst_C
#>
#>                  age  weight  height     bmi glucose cholesterol
#> age           1.0000  0.0312 -0.0187  0.0451  0.3021      0.4518
#> weight        0.0312  1.0000  0.1245  0.8712  0.2103      0.3215
#> height       -0.0187  0.1245  1.0000 -0.4523 -0.0312     -0.0198
#> bmi           0.0451  0.8712 -0.4523  1.0000  0.2456      0.3789
#> glucose       0.3021  0.2103 -0.0312  0.2456  1.0000      0.5124
#> cholesterol   0.4518  0.3215 -0.0198  0.3789  0.5124      1.0000
```

This 6x6 matrix contains correlations computed from data spread across all
three servers. The diagonal blocks (e.g., age-weight within inst_A) were
computed in plaintext. The off-diagonal blocks (e.g., age-glucose between
inst_A and inst_C) were computed via the MHE threshold decryption protocol.

### Accessing Correlation Components

The result object contains several useful fields:

```{r correlation-components, purl = FALSE}
# The full correlation matrix
cor_result$correlation

# Method used
cor_result$method
#> [1] "MHE-CKKS-Threshold"

# Within-server correlations (computed without encryption)
cor_result$local_correlations$inst_A
#>            age   weight
#> age     1.0000  0.0312
#> weight  0.0312  1.0000

# Cross-server correlations (computed via MHE)
cor_result$cross_correlations$inst_A_inst_C
#>            glucose cholesterol
#> age       0.3021      0.4518
#> weight    0.2103      0.3215

# MHE parameters used
cor_result$mhe_params
#> $log_n
#> [1] 12
#> $log_scale
#> [1] 40
```

### Interpreting Cross-Institution Correlations

```{r interpret-correlations, purl = FALSE}
# Extract the correlation matrix
R <- cor_result$correlation

# Within-institution correlations
cat("Within inst_A: cor(age, weight) =", round(R["age", "weight"], 3), "\n")
cat("Within inst_B: cor(height, bmi) =", round(R["height", "bmi"], 3), "\n")
cat("Within inst_C: cor(glucose, cholesterol) =", round(R["glucose", "cholesterol"], 3), "\n")

# Cross-institution correlations (the interesting ones!)
cat("\nCross-institution:\n")
cat("  cor(age, glucose)       [A x C] =", round(R["age", "glucose"], 3), "\n")
cat("  cor(bmi, cholesterol)   [B x C] =", round(R["bmi", "cholesterol"], 3), "\n")
cat("  cor(weight, bmi)        [A x B] =", round(R["weight", "bmi"], 3), "\n")
```

The cross-institution correlations are the key result: they reveal relationships
between variables held by different institutions, computed without either
institution seeing the other's raw data.

### Visualizing Correlations

```{r correlation-heatmap, fig.height=6, fig.width=7, purl=FALSE}
R <- cor_result$correlation
heatmap(
  R,
  symm = TRUE,
  col = colorRampPalette(c("#2166AC", "#F7F7F7", "#B2182B"))(50),
  margins = c(10, 10),
  main = "Cross-Institution Correlation Matrix\n(Computed via MHE Threshold Decryption)"
)
```

### A Note on Precision

Because CKKS is an *approximate* homomorphic encryption scheme, cross-server
correlation values have a small numerical error (typically on the order of
10^-3 to 10^-4). This is negligible for practical purposes but worth noting
when comparing with exact computations.

---

## Step 2: Principal Component Analysis (PCA)

### Understanding Distributed PCA

PCA finds linear combinations of variables that capture the maximum variance in
the data. When variables are spread across institutions, traditional PCA is
impossible because no single entity has access to all columns.

`ds.vertPCA` solves this by building on the privacy-preserving correlation
matrix from `ds.vertCor`. Since PCA on standardized data is mathematically
equivalent to eigen decomposition of the correlation matrix, we can:

1. Compute the correlation matrix via MHE (as shown above)
2. Perform eigen decomposition on the correlation matrix
3. Extract loadings (eigenvectors) and eigenvalues

### Performing PCA

```{r compute-pca, purl = FALSE}
pca_result <- ds.vertPCA(
  data_name = "D_aligned",
  variables = variables,
  n_components = 4,      # Return top 4 components (out of 6 possible)
  log_n = 12,
  datasources = connections
)
```

### Viewing PCA Results

The `print` method shows variance explained and the top loadings:

```{r print-pca, purl = FALSE}
print(pca_result)
#> Principal Component Analysis (Privacy-Preserving)
#> ==================================================
#>
#> Observations: 200
#> Variables: 6
#> Components: 4
#>
#> Variance Explained:
#>  Component Eigenvalue Percent Cumulative
#>        PC1     2.4312   40.52      40.52
#>        PC2     1.3187   21.98      62.50
#>        PC3     0.9823   16.37      78.87
#>        PC4     0.6102   10.17      89.04
#>
#> Loadings (top variables for first 2 PCs):
#>                 PC1    PC2
#> age           0.381  0.412
#> weight        0.452 -0.201
#> height       -0.112 -0.623
#> bmi           0.487  0.158
#> glucose       0.401  0.198
#> cholesterol   0.489 -0.102
```

### Interpreting PCA Output

The result contains three key pieces of information:

**Variance explained** -- how much of the total variance each component
captures:

```{r pca-variance, purl = FALSE}
data.frame(
  Component = paste0("PC", 1:4),
  Eigenvalue = round(pca_result$eigenvalues, 4),
  Percent = paste0(round(pca_result$variance_pct, 1), "%"),
  Cumulative = paste0(round(pca_result$cumulative_pct, 1), "%")
)
```

**Loadings** -- how much each variable contributes to each component. Values
close to +1 or -1 indicate strong contribution; values near 0 indicate weak
contribution. The sign indicates the direction of the relationship.

```{r pca-loadings, purl = FALSE}
loadings_df <- as.data.frame(round(pca_result$loadings, 3))
colnames(loadings_df) <- paste0("PC", 1:ncol(loadings_df))
loadings_df
```

A loading pattern where `bmi`, `weight`, `cholesterol`, and `glucose` all load
heavily on PC1 would suggest that PC1 captures a "metabolic health" dimension,
combining information from all three institutions.

**Note on scores:** `ds.vertPCA` does not return principal component scores
because computing them would require access to raw data across all servers.
If scores are needed, the loadings can be sent to each server for local
projection, which is a separate operation.

### Visualizing PCA

**Scree plot** -- shows how much variance each component explains:

```{r pca-scree, fig.height=5, fig.width=6, purl=FALSE}
barplot(
  pca_result$variance_pct,
  names.arg = paste0("PC", seq_along(pca_result$variance_pct)),
  main = "Scree Plot: Variance Explained by Component",
  ylab = "Percentage of Variance (%)",
  col = "steelblue",
  ylim = c(0, max(pca_result$variance_pct) * 1.3)
)
```

**Biplot of loadings** -- shows how variables relate to the first two
components:

```{r pca-biplot, fig.height=6, fig.width=6, purl=FALSE}
loadings <- pca_result$loadings

plot(loadings[, 1], loadings[, 2],
     xlim = c(-1, 1), ylim = c(-1, 1),
     xlab = paste0("PC1 (", round(pca_result$variance_pct[1], 1), "%)"),
     ylab = paste0("PC2 (", round(pca_result$variance_pct[2], 1), "%)"),
     main = "PCA Loading Plot\n(Variables from 3 Institutions)",
     pch = 19, col = "steelblue", cex = 1.5)

text(loadings[, 1], loadings[, 2],
     labels = pca_result$var_names,
     pos = 3, cex = 0.9)

abline(h = 0, v = 0, lty = 2, col = "gray60")

# Draw unit circle for reference
theta <- seq(0, 2 * pi, length.out = 100)
lines(cos(theta), sin(theta), lty = 3, col = "gray80")
```

Variables that cluster together in the loading plot are positively correlated.
Variables on opposite sides of the origin are negatively correlated. Variables
near the unit circle are well-represented by the first two components.

---

## Step 3: Generalized Linear Models (GLM)

### The Block Coordinate Descent Algorithm

`ds.vertGLM` fits GLMs using **Block Coordinate Descent (BCD)**, an iterative
algorithm designed for vertically partitioned data:

1. Divide predictors into "blocks" (one per institution)
2. Initialize all coefficients to zero
3. For each institution in turn:
   - Compute the linear predictor contribution from all *other* institutions
   - Update this institution's coefficients using local IRLS (Iteratively
     Reweighted Least Squares), treating the other contributions as an offset
   - Share the updated linear predictor (not raw data or coefficients)
4. Check convergence: if the total change in coefficients is below the
   tolerance, stop; otherwise return to step 3

Privacy is preserved because only the **linear predictor** (a single numeric
vector of length n) is shared between iterations. Raw data and intermediate
coefficients never leave their respective servers.

### Supported GLM Families

| Family             | Link     | Response Type       | Example                     |
|--------------------|----------|---------------------|-----------------------------|
| `gaussian`         | Identity | Continuous          | Blood pressure, BMI         |
| `binomial`         | Logit    | Binary (0/1)        | Disease status, mortality   |
| `poisson`          | Log      | Count               | Hospital visits, events     |
| `Gamma`            | Log      | Positive continuous | Healthcare costs, durations |
| `inverse.gaussian` | Log      | Positive continuous | Reaction times              |

### Example 1: Gaussian GLM (Linear Regression)

Predict blood pressure from predictors held at different institutions:

```{r glm-gaussian, purl = FALSE}
x_vars <- list(
  inst_A = c("age", "weight"),
  inst_B = c("bmi"),
  inst_C = c("glucose")
)

model_bp <- ds.vertGLM(
  data_name = "D_aligned",
  y_var = "outcome_bp",
  x_vars = x_vars,
  family = "gaussian",
  tol = 1e-5,
  verbose = FALSE,
  datasources = connections
)
```

Use `print` for a concise overview or `summary` for full details:

```{r glm-gaussian-print, purl = FALSE}
print(model_bp)
#> Vertically Partitioned GLM (Block Coordinate Descent)
#> =======================================================
#>
#> Call:
#> ds.vertGLM(data_name = "D_aligned", y_var = "outcome_bp",
#>     x_vars = x_vars, family = "gaussian", tol = 1e-05, verbose = FALSE,
#>     datasources = connections)
#>
#> Family: gaussian
#> Observations: 200
#> Predictors: 4
#> Regularization (lambda): 1e-04
#> Iterations: 23
#> Converged: TRUE
#>
#> Coefficients:
#>       age    weight       bmi   glucose
#>  0.389012  0.042318  0.571234  0.098745
```

```{r glm-gaussian-summary, purl = FALSE}
summary(model_bp)
#> Vertically Partitioned GLM - Summary
#> ====================================
#>
#> Call:
#> ds.vertGLM(data_name = "D_aligned", y_var = "outcome_bp",
#>     x_vars = x_vars, family = "gaussian", tol = 1e-05, verbose = FALSE,
#>     datasources = connections)
#>
#> Family: gaussian
#> Observations: 200
#> Predictors: 4
#> Regularization (lambda): 1e-04
#>
#> Convergence:
#>   Iterations: 23
#>   Converged: TRUE
#>
#> Deviance:
#>   Null deviance:     48312.1234  on 199 degrees of freedom
#>   Residual deviance: 12487.5678  on 196 degrees of freedom
#>
#> Model Fit:
#>   Pseudo R-squared (McFadden): 0.7415
#>   AIC: 12495.5678
#>
#> Coefficients:
#>          Estimate
#> age      0.389012
#> weight   0.042318
#> bmi      0.571234
#> glucose  0.098745
```

The summary includes:

- **Coefficients**: The estimated effect of each predictor. For a Gaussian GLM
  these are on the original scale (e.g., a one-unit increase in age is
  associated with a ~0.39 unit increase in blood pressure).
- **Deviance**: Measures model fit. The residual deviance should be
  substantially lower than the null deviance.
- **Pseudo R-squared (McFadden)**: Proportion of deviance explained by the
  model (1 - residual/null). Higher is better.
- **AIC**: Akaike Information Criterion for model comparison. Lower is better.

### Extracting Coefficients

Use the `coef` method to extract the coefficient vector:

```{r glm-coef, purl = FALSE}
coef(model_bp)
#>       age    weight       bmi   glucose
#>  0.389012  0.042318  0.571234  0.098745
```

### Example 2: Binomial GLM (Logistic Regression)

Predict diabetes risk from the same predictors:

```{r glm-binomial, purl = FALSE}
model_diabetes <- ds.vertGLM(
  data_name = "D_aligned",
  y_var = "outcome_diabetes",
  x_vars = x_vars,
  family = "binomial",
  tol = 1e-5,
  verbose = FALSE,
  datasources = connections
)

summary(model_diabetes)
```

For logistic regression, coefficients are on the **log-odds scale**. To get
odds ratios, exponentiate the coefficients:

```{r odds-ratios, purl = FALSE}
odds_ratios <- exp(coef(model_diabetes))
data.frame(
  Variable = names(odds_ratios),
  Coefficient = round(coef(model_diabetes), 4),
  OddsRatio = round(odds_ratios, 3)
)
```

An odds ratio greater than 1 means that a one-unit increase in the predictor is
associated with higher odds of diabetes. For example, an odds ratio of 1.05 for
`age` means each additional year of age increases the odds of diabetes by 5%.

### Example 3: Poisson GLM (Count Data)

Predict the number of hospital visits:

```{r glm-poisson, purl = FALSE}
model_visits <- ds.vertGLM(
  data_name = "D_aligned",
  y_var = "outcome_visits",
  x_vars = list(
    inst_A = c("age"),
    inst_B = c("bmi")
  ),
  family = "poisson",
  tol = 1e-5,
  verbose = FALSE,
  datasources = connections
)

summary(model_visits)
```

For Poisson regression, coefficients are on the **log scale**. Exponentiating
gives **rate ratios**:

```{r rate-ratios, purl = FALSE}
rate_ratios <- exp(coef(model_visits))
data.frame(
  Variable = names(rate_ratios),
  Coefficient = round(coef(model_visits), 4),
  RateRatio = round(rate_ratios, 3)
)
```

A rate ratio of 1.02 for `age` means each additional year of age is associated
with a 2% increase in the expected number of hospital visits.

### Example 4: Gamma GLM (Positive Continuous Data)

Predict healthcare costs. The Gamma family is appropriate for strictly positive,
right-skewed outcomes where variance increases with the mean:

```{r glm-gamma, purl = FALSE}
model_cost <- ds.vertGLM(
  data_name = "D_aligned",
  y_var = "outcome_cost",
  x_vars = list(
    inst_A = c("age"),
    inst_B = c("bmi")
  ),
  family = "Gamma",
  tol = 1e-5,
  verbose = FALSE,
  datasources = connections
)

summary(model_cost)
```

### Comparing Models

When you have fitted multiple models, you can compare them side by side:

```{r compare-models, purl = FALSE}
models <- list(
  "Blood Pressure (gaussian)"  = model_bp,
  "Diabetes Risk (binomial)"   = model_diabetes,
  "Hospital Visits (poisson)"  = model_visits,
  "Healthcare Cost (Gamma)"    = model_cost
)

comparison <- data.frame(
  Model      = names(models),
  Family     = sapply(models, function(m) m$family),
  Predictors = sapply(models, function(m) m$n_vars),
  Iterations = sapply(models, function(m) m$iterations),
  Deviance   = sapply(models, function(m) round(m$deviance, 2)),
  Pseudo_R2  = sapply(models, function(m) round(m$pseudo_r2, 4)),
  AIC        = sapply(models, function(m) round(m$aic, 2)),
  Converged  = sapply(models, function(m) m$converged),
  row.names  = NULL
)

comparison
```

### Convergence Considerations

The BCD algorithm typically converges in 20--100 iterations. If a model does
not converge, consider:

- **Increasing `max_iter`**: The default is 100, which is sufficient for most
  problems. Set it higher for difficult cases.
- **Relaxing `tol`**: The default tolerance is 1e-6. A value of 1e-5 or 1e-4
  may be sufficient for exploratory analysis.
- **Checking data quality**: Highly collinear predictors or extreme outliers can
  slow convergence. The `lambda` parameter (default 1e-4) provides L2
  regularization that helps with collinearity.

```{r convergence-check, purl = FALSE}
# Check convergence details
cat("Converged:", model_bp$converged, "\n")
cat("Iterations:", model_bp$iterations, "\n")
cat("Lambda (regularization):", model_bp$lambda, "\n")
```

---

## Privacy Summary

Throughout all analyses, privacy is maintained:

| Analysis    | What Is Shared                             | What Stays Private               |
|-------------|-------------------------------------------|----------------------------------|
| Correlation | Encrypted columns (CKKS ciphertexts), partial decryption shares | Raw variable values              |
| PCA         | Same as correlation (PCA is built on it)   | Raw variable values              |
| GLM         | Linear predictor vector (eta = X * beta)   | Raw data, coefficients during iteration |

**No raw patient-level data ever leaves any institution.** Cross-server
correlation uses MHE with threshold decryption, meaning no single server (and
not even the client) can decrypt without cooperation from all parties.

For a detailed mathematical treatment of these methods, see the
[Methodology](c-methodology.html) vignette.

## Cleanup

When you are finished with the analysis, log out of all connections:

```{r cleanup, purl = FALSE}
datashield.logout(connections)
```
