---
title: "Getting Started with dsVertClient"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with dsVertClient}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE, purl = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#>",
  eval = FALSE,
  purl = FALSE
)
```

## What is Vertical Data Partitioning?

In traditional **horizontal partitioning** (the standard DataSHIELD scenario),
different institutions hold data for *different patients* with the *same
variables*. For example, Hospital A has 1,000 patients and Hospital B has 2,000
different patients, but both measure the same things (age, blood pressure, lab
results, etc.).

**Vertical partitioning** is fundamentally different: multiple institutions hold
data for the *same patients* but with *different variables*. This commonly
occurs when:

- A hospital has clinical data (diagnoses, treatments, vital signs)
- A laboratory has biomarker measurements (glucose, cholesterol, HbA1c)
- A research center has genomic or imaging data
- A government registry has demographic and socioeconomic data

All of these institutions have records for the same individuals, linked by a
common identifier (e.g., national health number), but each holds different
pieces of information.

### Illustrative Example

Consider three institutions that each hold data for the same set of patients:

| Institution | Role | Variables |
|-------------|------|-----------|
| **Hospital A** | Clinical care | age, weight, blood pressure |
| **Laboratory B** | Lab testing | glucose, cholesterol, HbA1c |
| **Research Center C** | Genomics | gene_score_1, gene_score_2 |

A researcher wants to study whether genetic markers (at Center C) are
associated with metabolic outcomes (at Lab B), controlling for demographics (at
Hospital A). Answering this question requires combining variables from all three
institutions -- but the data cannot be pooled due to privacy regulations.

## The Privacy Challenge

To analyze relationships between variables held by different institutions (e.g.,
"Does glucose level correlate with genetic markers?"), the traditional approach
would be to:

1. Send all data to a central location
2. Merge the datasets by patient identifier
3. Perform the analysis on the combined table

This centralization approach raises serious concerns:

- **Patient privacy**: Sensitive health data leaves its protected environment
- **Legal barriers**: GDPR, HIPAA, and other regulations restrict data transfers
- **Trust issues**: Institutions may not trust each other (or a central party) with their data
- **Data governance**: Each institution may have different consent frameworks

## The DataSHIELD Solution

[DataSHIELD](https://www.datashield.org/) is a framework for privacy-preserving
federated analysis. Its key principle is:

> *"Bring the analysis to the data, not the data to the analysis."*

Instead of moving data, DataSHIELD:

1. Sends analysis commands to each server
2. Each server executes computations locally on its own data
3. Only **non-disclosive aggregate results** are returned to the analyst
4. The analyst's client combines these aggregates to produce the final result

**dsVertClient** extends DataSHIELD specifically for **vertically partitioned
data**, adding three capabilities that standard DataSHIELD does not provide:

- **Privacy-preserving record alignment** via cryptographic hashing
- **Cross-server correlation and PCA** using Multiparty Homomorphic Encryption (MHE)
- **Distributed GLM fitting** using Block Coordinate Descent (BCD)

## Prerequisites

Before using dsVertClient, you need:

1. **R (>= 4.0.0)** installed on your local machine
2. **dsVertClient** installed (the client-side package you run locally)
3. **dsVert** installed on each Opal server (the server-side companion package)
4. **Opal servers** set up and accessible, each hosting one data partition
5. Login credentials (username, password, URL) for each Opal server

Install the client package from GitHub:

```{r install, eval=FALSE, purl=FALSE}
# install.packages("remotes")
remotes::install_github("isglobal-brge/dsVertClient")
```

---

## Connecting to Opal Servers

In a real deployment, each institution runs an
[Opal](https://www.obiba.org/pages/products/opal/) server that hosts its data
behind a firewall. The analyst connects to all servers from a local R session
using the DSI (DataSHIELD Interface) package.

### Building Login Credentials

Use `DSI::newDSLoginBuilder()` to assemble the connection details for each
server. Each entry specifies the server name, its URL, the table containing the
data, and authentication credentials:

```{r build-login, purl = FALSE}
library(dsVertClient)
library(DSI)

builder <- DSI::newDSLoginBuilder()

builder$append(
  server = "hospital_A",
  url = "https://opal-hospital-a.example.org",
  user = "analyst",
  password = "secret_A",
  table = "project.clinical_data"
)

builder$append(
  server = "lab_B",
  url = "https://opal-lab-b.example.org",
  user = "analyst",
  password = "secret_B",
  table = "project.lab_results"
)

builder$append(
  server = "research_C",
  url = "https://opal-research-c.example.org",
  user = "analyst",
  password = "secret_C",
  table = "project.genomic_data"
)
```

### Logging In

The `datashield.login()` call establishes connections to all servers
simultaneously and assigns the data to a server-side symbol (here `"D"`):

```{r login, purl = FALSE}
connections <- DSI::datashield.login(
  logins = builder$build(),
  assign = TRUE,
  symbol = "D"
)
```

After this call, each server has a data frame called `D` in its R session. The
analyst never sees the raw data -- only the server-side symbol name.

---

## Step 1: Validate Identifier Formats

Before aligning records across servers, it is important to verify that patient
identifiers have a consistent format at every institution. Common pitfalls
include:

- Different ID formats across institutions (e.g., `"001"` vs `"1"` vs `"P-001"`)
- Leading or trailing whitespace
- Missing values or duplicate identifiers
- Type mismatches (numeric vs character)

The `ds.validateIdFormat()` function checks all of these without revealing the
actual identifier values:

```{r validate-ids, purl = FALSE}
validation <- ds.validateIdFormat(
  data_name = "D",
  id_col = "patient_id",
  datasources = connections
)

print(validation)
```

The output reports, for each server:

- **n_obs**: Total number of records
- **n_unique**: Number of unique identifiers (should equal n_obs)
- **n_missing**: Number of missing identifiers (should be 0)
- **format_signature**: A hashed summary of the ID format (should match across servers)

If the `valid` field is `TRUE`, formats are consistent and you can proceed to
alignment. If issues are detected, the `warnings` field will describe what needs
to be fixed.

You can also validate against a specific pattern:

```{r validate-pattern, purl = FALSE}
validation <- ds.validateIdFormat(
  data_name = "D",
  id_col = "patient_id",
  pattern = "^PATIENT_[0-9]{5}$",
  datasources = connections
)
```

---

## Step 2: Privacy-Preserving Record Alignment

### Why Alignment is Needed

Even though all institutions have data for the same patients, the records are
stored in different orders. Institution A might have patient "P042" at row 1,
while Institution B has the same patient at row 157. To compute cross-server
statistics (e.g., correlation between age at Hospital A and glucose at Lab B),
corresponding records must be in the same row position at every server.

**But we cannot simply share patient IDs between servers.** That would expose
potentially sensitive identifiers across institutional boundaries.

### The Solution: Cryptographic Hashing

dsVertClient aligns records using SHA-256 cryptographic hashing:

1. One server is designated as the **reference** server
2. The reference server hashes all of its patient identifiers using SHA-256
3. The hashed values are sent to the other servers (hashes are one-way -- the original IDs cannot be recovered)
4. Each server hashes its own identifiers, matches them against the reference hashes, and reorders its data accordingly
5. After alignment, all servers have records in the same order, with only the shared patients retained

| Original ID | SHA-256 Hash |
|-------------|--------------|
| `PATIENT_00042` | `a7f3b9c2d8e1f4a5...` (64 hex characters) |

Given only the hash, recovering the original identifier is computationally
infeasible.

### Step 2a: Obtain Reference Hashes

Choose one server as the reference and retrieve its hashed identifiers:

```{r hash-ids, purl = FALSE}
ref_hashes <- ds.hashId(
  data_name = "D",
  id_col = "patient_id",
  algo = "sha256",
  datasource = connections["hospital_A"]
)

# ref_hashes$n      -- number of hashed identifiers
# ref_hashes$hashes -- character vector of SHA-256 hashes
```

### Step 2b: Align All Servers

Send the reference hashes to all servers (including the reference server itself).
Each server will:

1. Hash its own identifiers using the same algorithm
2. Match them against the reference hashes
3. Reorder its data to match the reference order
4. Drop any records not present in the reference set
5. Store the result in a new server-side data frame

```{r align-records, purl = FALSE}
ds.alignRecords(
  data_name = "D",
  id_col = "patient_id",
  reference_hashes = ref_hashes$hashes,
  newobj = "D_aligned",
  datasources = connections
)
```

The function prints alignment statistics for each server, for example:

```
Server 'hospital_A': 200 of 200 records matched (100.0%)
Server 'lab_B': 195 of 200 records matched (97.5%)
Server 'research_C': 198 of 200 records matched (99.0%)
```

If a server matches fewer than 100% of records, it means some reference patients
are not present at that institution. Only the intersection (patients present at
all institutions) is retained.

After alignment, every server has a data frame called `D_aligned` with:

- The **same number of rows**
- Rows in the **same order** (each row position corresponds to the same patient)
- Ready for **cross-server statistical analysis**

---

## Why Multiparty Homomorphic Encryption (MHE) is Needed

With records aligned, you might wonder: can we simply compute statistics by
exchanging summary values between servers? For some analyses, such as fitting a
GLM via Block Coordinate Descent, sharing aggregate linear predictor
contributions is sufficient and does not reveal individual-level data.

However, for **cross-server correlation and PCA**, the situation is more
delicate. Computing the correlation between a variable on Server A and a
variable on Server B fundamentally requires combining information about
individual observations across servers. Standard summary statistics (means,
variances) computed on each server separately are not enough -- the cross-server
covariance requires knowledge of paired values.

dsVertClient solves this using **Multiparty Homomorphic Encryption (MHE)** with
threshold decryption. The protocol works as follows:

1. **Key generation**: Each server generates a secret key share and a public key
   share. No single server (or the analyst) holds the complete decryption key.

2. **Collective public key**: The public key shares are combined into a
   Collective Public Key (CPK). Data encrypted under the CPK can only be
   decrypted when *all* servers cooperate.

3. **Encrypted computation**: Each server standardizes its data, encrypts the
   columns under the CPK, and shares the ciphertexts. Another server can then
   compute the encrypted cross-product (correlation numerator) homomorphically
   -- that is, by operating directly on ciphertexts without ever decrypting the
   data.

4. **Threshold decryption**: The encrypted result is partially decrypted by each
   server using its secret key share. The analyst collects all partial shares
   and fuses them to recover only the final aggregate statistic (the correlation
   coefficient). No individual data values are ever revealed.

This approach provides strong security guarantees:

- **No single point of trust**: Even the analyst cannot decrypt individual data
- **Collusion resistance**: Any K-1 colluding servers cannot decrypt without the K-th server
- **Minimal disclosure**: Only the final correlation coefficients are revealed

The `ds.vertCor()` and `ds.vertPCA()` functions handle this entire MHE protocol
automatically. From the analyst's perspective, the interface is straightforward:

```{r mhe-example, purl = FALSE}
# Define which variables are on which server
variables <- list(
  hospital_A  = c("age", "weight"),
  lab_B       = c("glucose", "cholesterol"),
  research_C  = c("gene_score_1", "gene_score_2")
)

# Compute the full 6x6 correlation matrix across all servers
cor_result <- ds.vertCor(
  data_name = "D_aligned",
  variables = variables,
  datasources = connections
)

print(cor_result)
```

The GLM function, `ds.vertGLM()`, uses a different approach (Block Coordinate
Descent) that does not require MHE, because it only shares linear predictor
contributions rather than encrypted individual values:

```{r glm-example, purl = FALSE}
model <- ds.vertGLM(
  data_name = "D_aligned",
  y_var = "outcome_bp",
  x_vars = list(
    hospital_A = c("age", "weight"),
    lab_B      = c("glucose"),
    research_C = c("gene_score_1")
  ),
  family = "gaussian",
  datasources = connections
)

summary(model)
```

---

## Logging Out

When your analysis is complete, close all server connections:

```{r logout, purl = FALSE}
DSI::datashield.logout(connections)
```

---

## Summary of the Workflow

A typical dsVertClient analysis follows these steps:

1. **Connect** to Opal servers using `DSI::datashield.login()`
2. **Validate** identifier formats with `ds.validateIdFormat()`
3. **Hash** identifiers on a reference server with `ds.hashId()`
4. **Align** records across all servers with `ds.alignRecords()`
5. **Analyze** using `ds.vertCor()`, `ds.vertPCA()`, or `ds.vertGLM()`
6. **Disconnect** with `DSI::datashield.logout()`

## Further Reading

- **[Statistical Analysis](b-statistical-analysis.html)**: Detailed examples of
  correlation, PCA, and GLM (Gaussian, binomial, Poisson, Gamma, inverse
  Gaussian) with interpretation guidance and visualizations.

- **[Methodology](c-methodology.html)**: Mathematical foundations of Block
  Coordinate Descent for GLMs, the MHE threshold decryption protocol for
  correlation/PCA, and the security model.
